---
# Feel free to add content and custom Front Matter to this file.
# To modify the layout, see https://jekyllrb.com/docs/themes/#overriding-theme-defaults


author_profile: true
classes: wide
layout: archive
---

<!-- bundle exec jekyll serve -->
<!-- http://localhost:4000 -->

I am a PhD candidate in [Computer Science Department](http://cs.umd.edu/) at [University of Maryland, College Park](http://www.umd.edu), 
working with Professor [Hernisa Kacorri](https://terpconnect.umd.edu/~hernisa/). I am a member of [Intelligent Assistive Machines lab](https://iam.umd.edu/), 
[HCIL](http://hcil.umd.edu), and [TRACE center](https://trace.umd.edu/) at UMD.
My research interest is about addressing the real-world problems using human-computer interaction and machine learning techniques. 
I build machine learning applications and investigate interactions between a user and the application using quantitative and qualitative analysis.
In my recent projects, I explored the challenges of identifying errors from speech recognizer and 
conducted user studies to facilitate machine teaching with a teachable object recognizer for non-experts.

<b> I am planning to graduate by February 2021 and I am looking for research positions in academia and industry. </b> 

<!-- ![intro_image](/images/symposium.png){:class="img-responsive"} -->
<!-- <img src="/images/Jonggi_presentation.png" width="100%"> -->


## [Projects]({{ site.baseurl }}{% link projects.md %})

The overarching theme of my research is designing and evaluating accessible interactions with emerging technologies such as wearables and AI-infused applications. 

<table style="table-layout: fixed;width: 100%;border:none;">
	<tr>
		<td style="border:none;"><a href="projects/index.html#teachable-interface-for-personalizing-an-object-recognizer"><img src="/images/MachineTeachingPerception.png"></a></td>
		<td style="border:none;"><a href="projects/index.html#identifying-speech-recognition-errors-with-audio-only-interactions"><img src="/images/dictation_CHI2017.png"></a></td>
	</tr>
	<tr>
		<td style="border:none;font-size:1vw;vertical-align:top;"><a href="projects/index.html#teachable-interface-for-personalizing-an-object-recognizer">Teachable interface for personalizing an object recognizer.</a></td>
		<td style="border:none;font-size:1vw;vertical-align:top;"><a href="projects/index.html#identifying-speech-recognition-errors-with-audio-only-interactions">Identifying speech recognition errors with audio-only interactions</a></td>
	</tr>
	<tr>
		<td style="border:none;"><a href="projects/index.html#detecting-of-misalignments-between-promotion-links-and-landing-pages"><img src="/images/link_analysis.png"></a></td>
		<td style="border:none;"><a href="projects/index.html#interacting-with-mobile-and-wearable-devices"><img src="/images/Haptic_GI2016.png"></a></td>
	</tr>
	<tr>
		<td style="border:none;font-size:1vw;vertical-align:top;"><a href="projects/index.html#detecting-of-misalignments-between-promotion-links-and-landing-pages">Detecting of misalignments between promotion links and landing pages</a></td>
		<td style="border:none;font-size:1vw;vertical-align:top;"><a href="projects/index.html#interacting-with-mobile-and-wearable-devices">Interacting with wearable devices</a></td>
	</tr>
</table>

## [Recent publications]({{ site.baseurl }}{% link publications.md %})

[Reviewing Speech Input with Audio: Differences between Blind and Sighted Users](/papers/TACCESS2020-speech.pdf)<br>
**Jonggi Hong**, Christine Vaing, Hernisa Kacorri, Leah Findlater. TACCESS 2020.

[Crowdsourcing the Perception of Machine Teaching](/papers/CHI2020-CrowdTeaMa.pdf)<br>
**Jonggi Hong**, Kyungjun Lee, June Xu, Hernisa Kacorri. CHI 2020.

[Revisiting Blind Photography in the Context of Teachable Object Recognizers](/papers/ASSETS2019-Revisiting.pdf)<br>
Kyungjun Lee, **Jonggi Hong**, Ebrima Jarjue, Simone Pimento, Hernisa Kacorri. ASSETS 2019

[Identifying Speech Input Errors Through Audio-Only Interaction](/papers/CHI2018-DictationErrorsAudioOnly.pdf)<br>
**Jonggi Hong**, Leah Findlater. CHI 2018.